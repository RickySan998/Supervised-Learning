{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example\n",
    "# train_data_filename = 'dtrain123.dat'\n",
    "# test_data_filename = 'dtest123.dat'\n",
    "\n",
    "# train_data = np.loadtxt(train_data_filename)\n",
    "# test_data = np.loadtxt(test_data_filename)\n",
    "\n",
    "# train_labels = train_data[:, 0] - 1; train_samples = train_data[:, 1:]\n",
    "# test_labels = test_data[:,0] - 1; test_samples = test_data[:,1:]\n",
    "def traingen2(train_labels, mistake_counters, num_classes, kernel_mat_train):\n",
    "    \n",
    "    mistakes = 0\n",
    "    for i in range(train_labels.shape[0]):\n",
    "        \n",
    "        pred_arg = (kernel_mat_train[i,:].T @ mistake_counters).squeeze()\n",
    "        \n",
    "        pred_class = np.argmax(pred_arg)\n",
    "        label = int(train_labels[i])\n",
    "        \n",
    "        label_vect = np.ones(num_classes) * -1\n",
    "        label_vect[label] = 1\n",
    "        \n",
    "        class_mistakes = (pred_arg * label_vect) <= 0\n",
    "        \n",
    "        mistake_counters[i, class_mistakes] -= (2*(pred_arg[class_mistakes] > 0) - 1)\n",
    "        \n",
    "        if pred_class != label:\n",
    "            mistakes += 1\n",
    "            \n",
    "    return mistake_counters, mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full data\n",
    "path_real_data = \"./zipcombo.dat\"\n",
    "data = np.loadtxt(path_real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = data[:, 1:]; data_labels = data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9298, 256)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "def polynomial_kernel(train_dat, test_pt, p):\n",
    "    \"\"\"\n",
    "    Computes the polynomial kernel for a given order\n",
    "    Input:\n",
    "    train_dat = m x dim array\n",
    "    test_pt = n x dim vector\n",
    "    p = polynomial order, scalar\n",
    "    \n",
    "    Output:\n",
    "    result = m x n array, where result[i,j] = K(x_train[i,:], test_pt[j,:])\n",
    "    \n",
    "    Note: To allow bulk evaluation for efficiency\n",
    "    \"\"\"\n",
    "    return ((train_dat @ test_pt.T)**p)\n",
    "\n",
    "def init_mistake_counters(train_data, num_class):\n",
    "    \"\"\"\n",
    "    To use kernels, need only to keep track of the number of times during training (for each perceptron)\n",
    "    how many times a sample is predicted wrongly (since this many times would its featurised vector be added\n",
    "    to the 'weight'). This avoids explicitly computing the weight vectors because it needs the feature map computation\n",
    "    which is not feasible if e.g. gaussian kernel. Furthermore, the predictions later depends on the polarity of the label\n",
    "    hence our 'counters' is also multiplied by the sample label\n",
    "    \n",
    "    Input:\n",
    "    train_data: only to get number of points\n",
    "    \n",
    "    Output:\n",
    "    matrix of zeros, size = m x num_class. Because keep track for every perceptron\n",
    "    \"\"\"\n",
    "    m = train_data.shape[0]\n",
    "    return np.zeros((m, num_class))\n",
    "\n",
    "def class_predict(mistake_counters, kernel_mat):\n",
    "    \"\"\"\n",
    "    Note: y_hat will be vector of -1s and only one 1 at the corresponding class.\n",
    "    \n",
    "    Input:\n",
    "    data_pt = 1 x dim vector to predict\n",
    "    mistake_counters as above\n",
    "    kernel_mat = m x 1 array where K[i] = K(xi, data_pt)\n",
    "    \n",
    "    Output:\n",
    "    predictions: vector of -1s and one 1 at the predicted class\n",
    "    \"\"\"\n",
    "    num_class = mistake_counters.shape[1]\n",
    "    \n",
    "    inner_term = mistake_counters * kernel_mat\n",
    "    inner_term = np.sum(inner_term, axis = 0).reshape(-1)\n",
    "    \n",
    "    print(inner_term)\n",
    "    \n",
    "    pred_class = np.argmax(inner_term)\n",
    "    \n",
    "    predictions = -1* np.ones((num_class))\n",
    "    predictions[pred_class] = 1\n",
    "    \n",
    "    return inner_term, predictions\n",
    "\n",
    "def class_predict2(mistake_counters, kernel_mat):\n",
    "    \"\"\"\n",
    "    Note: y_hat will be vector of -1s and only one 1 at the corresponding class.\n",
    "    \n",
    "    Input:\n",
    "    data_pt = 1 x dim vector to predict\n",
    "    mistake_counters as above\n",
    "    kernel_mat = m x n_test array where K[i,j] = K(xi, test_pt[j])\n",
    "    \n",
    "    Output:\n",
    "    predictions: vector of arguments\n",
    "    \"\"\"\n",
    "    inner_term = kernel_mat.T @ mistake_counters\n",
    "    \n",
    "    return inner_term\n",
    "\n",
    "def split_train_test(data_samples, data_labels, train_prop, test_prop, seed = 88):\n",
    "    \"\"\"\n",
    "    Split to 80% train and 20% test as required, randomly by first shuffling\n",
    "    \"\"\"\n",
    "    n_tot = data_samples.shape[0]; n_train = round(train_prop * n_tot)\n",
    "    permute = np.random.permutation(n_tot)\n",
    "    \n",
    "    data_shuffled = data_samples[permute,:]\n",
    "    data_shuffled_labels = data_labels[permute]\n",
    "    \n",
    "    train_samples = data_shuffled[:n_train, :]; train_labels = data_shuffled_labels[:n_train]\n",
    "    test_samples = data_shuffled[n_train:, :]; test_labels = data_shuffled_labels[n_train:]\n",
    "    \n",
    "    return train_samples, train_labels, test_samples, test_labels\n",
    "\n",
    "def split_train_test2(data_samples, data_labels, train_prop, test_prop, seed = 88):\n",
    "    \"\"\"\n",
    "    Split to 80% train and 20% test as required, randomly by first shuffling\n",
    "    \"\"\"\n",
    "    n_tot = data_samples.shape[0]; n_train = round(train_prop * n_tot)\n",
    "    permute = np.random.permutation(n_tot)\n",
    "    \n",
    "    data_shuffled = data_samples[permute,:]\n",
    "    data_shuffled_labels = data_labels[permute]\n",
    "    \n",
    "    train_samples = data_shuffled[:n_train, :]; train_labels = data_shuffled_labels[:n_train]\n",
    "    test_samples = data_shuffled[n_train:, :]; test_labels = data_shuffled_labels[n_train:]\n",
    "    \n",
    "    return train_samples, train_labels, test_samples, test_labels, data[:n_train, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "def traingen(train_data, train_labels, mistake_counters, num_classes, kernel_mat_train):\n",
    "    \"\"\"\n",
    "    Performs the num_classes perceptron training for ONE epoch\n",
    "    \n",
    "    Returns:\n",
    "    the updated mistake counters and the number of mistakes in that epoch\n",
    "    \"\"\"\n",
    "    # To reduce recomputation, compute the kernel mat for all pairs of training data outside. Since this don't change\n",
    "    # every epoch\n",
    "    \n",
    "    # initialize mistakes\n",
    "    mistakes = 0\n",
    "    \n",
    "    # Iterate through each data point in an 'online' fashion\n",
    "    m, _ = train_data.shape\n",
    "    \n",
    "    for i in range(m):\n",
    "        kernel_mat = kernel_mat_train[:,i].reshape(-1,1)\n",
    "        label = int(train_labels[i]); label_vect = -1 * np.ones((num_classes)); label_vect[label] = 1\n",
    "        \n",
    "        # Prediction vector\n",
    "#         pred_arg = class_predict2(mistake_counters, kernel_mat)\n",
    "        pred_arg = kernel_mat.T @ mistake_counters\n",
    "        \n",
    "        # Since training processes data one at a time, we can squeeze dimension\n",
    "        pred_arg = pred_arg.reshape(-1)\n",
    "        \n",
    "        pred_class = np.argmax(pred_arg)\n",
    "        class_mistakes = (pred_arg * label_vect) <= 0\n",
    "        \n",
    "        # If mistake for that perceptron, update the corresponding mistake counter. Check each perceptron independently\n",
    "        # So if mistake on a perceptron, update only that perceptron\n",
    "        \n",
    "        # Subtlety: at 0,0,0..0 case, we treat the perceptron as predicting -1 for ALL (this case should never happen\n",
    "        # in subsequent iterations). i.e. sign(0) is treated as -1. Furthermore, the 0,0,..0 case will always be treated\n",
    "        # as all perceptrons making a mistake (even though if all -1, then all but one should be correct). Finally, if all\n",
    "        # 3 perceptrons outputs are equal, we treat it as predicting the first class\n",
    "        \n",
    "        mistake_counters[i,class_mistakes] -=  (2*(pred_arg[class_mistakes] > 0) - 1)\n",
    "        \n",
    "        if pred_class != label:\n",
    "            mistakes += 1\n",
    "        \n",
    "    return mistake_counters, mistakes\n",
    "\n",
    "def testclassifiers(train_samples, test_samples, test_labels, mistake_counters, num_classes, kernel_mat):\n",
    "        \"\"\"\n",
    "        Predict on the whole test sample batch\n",
    "        \"\"\"\n",
    "        # Get shapes\n",
    "        n_test = test_samples.shape[0]\n",
    "        \n",
    "        # Compute kernel matrix outside, again don't change per epoch\n",
    "        \n",
    "        # Bulk predict on the whole test labels\n",
    "        inner_term = class_predict2(mistake_counters, kernel_mat)\n",
    "        \n",
    "        # Class predictions\n",
    "        class_pred = np.argmax(inner_term, axis = 1)\n",
    "        \n",
    "        # Count mistakes\n",
    "        mistakes = np.sum(test_labels != class_pred)\n",
    "        \n",
    "        # Return average mistakes\n",
    "        return mistakes / n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traingen23(data, classifiers,numb_class , d,kernel_result):\n",
    "    mistakes = 0\n",
    "    preds = np.zeros((numb_class))\n",
    "    maxi = 0\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "     \n",
    "        preds = (classifiers@kernel_result[i].reshape(-1,1)).T[0]\n",
    "\n",
    "        #print(\"preds\",preds.shape)\n",
    "\n",
    "        y_pred = np.argmax(preds)\n",
    "        y_true = int(data[i,0])\n",
    "\n",
    "        Y_true = np.ones(numb_class)*-1\n",
    "        Y_true[y_true] = 1\n",
    "\n",
    "        y_mistakes = ((preds*Y_true) <= 0)\n",
    "        #print(y_mistakes.shape)\n",
    "\n",
    "        classifiers[y_mistakes, i] = classifiers[y_mistakes, i] - mysign2(preds[y_mistakes])  \n",
    "\n",
    "        if y_pred != y_true:\n",
    "            mistakes+=1\n",
    "        \n",
    "            \n",
    "    return mistakes, classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1\n",
      "Epoch 0, made 6645 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 1, made 5754 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 2, made 4919 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 3, made 4323 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 4, made 3839 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 5, made 3504 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 6, made 3158 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 7, made 2950 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 8, made 2697 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 9, made 2542 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "5.395120143890381\n"
     ]
    }
   ],
   "source": [
    "num_iter = 1\n",
    "start = time.time()\n",
    "d = 3\n",
    "for it in range(num_iter):\n",
    "    print(f\"Iteration {it+1}/{num_iter}\")\n",
    "    train_samples, train_labels, test_samples, test_labels, train = split_train_test2(data_samples, data_labels, 0.8, 0.2)\n",
    "    num_classes = 10\n",
    "    mistake_counters = init_mistake_counters(train_samples, num_classes).T\n",
    "    \n",
    "    # Pre-compute outside for speed\n",
    "    kernel_mat_train = polynomial_kernel(train_samples, train_samples, d)\n",
    "    kernel_mat_test = polynomial_kernel(train_samples, test_samples, d)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        mistakes, mistake_counters = traingen23(train, mistake_counters,10,d,kernel_mat_train)\n",
    "#         test_rate = testclassifiers(train_samples, test_samples, test_labels, mistake_counters, num_classes, kernel_mat_test)\n",
    "        print(f\"Epoch {epoch}, made {mistakes} mistakes on training data and mistake rate of {test_rate} on test data\")\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1\n",
      "Epoch 0, made 632 mistakes on training data and mistake rate of 0.04516129032258064 on test data\n",
      "Epoch 1, made 132 mistakes on training data and mistake rate of 0.04032258064516129 on test data\n",
      "Epoch 2, made 64 mistakes on training data and mistake rate of 0.03387096774193549 on test data\n",
      "Epoch 3, made 30 mistakes on training data and mistake rate of 0.03333333333333333 on test data\n",
      "Epoch 4, made 20 mistakes on training data and mistake rate of 0.032795698924731186 on test data\n",
      "Epoch 5, made 15 mistakes on training data and mistake rate of 0.031720430107526884 on test data\n",
      "Epoch 6, made 10 mistakes on training data and mistake rate of 0.02795698924731183 on test data\n",
      "Epoch 7, made 11 mistakes on training data and mistake rate of 0.025806451612903226 on test data\n",
      "Epoch 8, made 5 mistakes on training data and mistake rate of 0.024193548387096774 on test data\n",
      "Epoch 9, made 7 mistakes on training data and mistake rate of 0.026344086021505377 on test data\n",
      "5.8558189868927\n"
     ]
    }
   ],
   "source": [
    "num_iter = 1\n",
    "d = 3\n",
    "\n",
    "start = time.time()\n",
    "for it in range(num_iter):\n",
    "    print(f\"Iteration {it+1}/{num_iter}\")\n",
    "    train_samples, train_labels, test_samples, test_labels = split_train_test(data_samples, data_labels, 0.8, 0.2)\n",
    "    num_classes = 10\n",
    "    mistake_counters = init_mistake_counters(train_samples, num_classes)\n",
    "    \n",
    "    # Pre-compute outside for speed\n",
    "    kernel_mat_train = polynomial_kernel(train_samples, train_samples, d)\n",
    "    kernel_mat_test = polynomial_kernel(train_samples, test_samples, d)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        mistake_counters, mistakes = traingen(train_samples, train_labels, mistake_counters, num_classes, kernel_mat_train)\n",
    "        test_rate = testclassifiers(train_samples, test_samples, test_labels, mistake_counters, num_classes, kernel_mat_test)\n",
    "        print(f\"Epoch {epoch}, made {mistakes} mistakes on training data and mistake rate of {test_rate} on test data\")\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1\n",
      "Epoch 0, made 590 mistakes on training data and mistake rate of 0.0553763440860215 on test data\n",
      "Epoch 1, made 122 mistakes on training data and mistake rate of 0.053763440860215055 on test data\n",
      "Epoch 2, made 63 mistakes on training data and mistake rate of 0.043548387096774194 on test data\n",
      "Epoch 3, made 35 mistakes on training data and mistake rate of 0.043010752688172046 on test data\n",
      "Epoch 4, made 19 mistakes on training data and mistake rate of 0.04032258064516129 on test data\n",
      "Epoch 5, made 16 mistakes on training data and mistake rate of 0.038172043010752686 on test data\n",
      "Epoch 6, made 5 mistakes on training data and mistake rate of 0.03333333333333333 on test data\n",
      "Epoch 7, made 6 mistakes on training data and mistake rate of 0.03602150537634408 on test data\n",
      "Epoch 8, made 5 mistakes on training data and mistake rate of 0.03870967741935484 on test data\n",
      "Epoch 9, made 7 mistakes on training data and mistake rate of 0.03494623655913978 on test data\n",
      "5.765453815460205\n"
     ]
    }
   ],
   "source": [
    "num_iter = 1\n",
    "start = time.time()\n",
    "\n",
    "for it in range(num_iter):\n",
    "    print(f\"Iteration {it+1}/{num_iter}\")\n",
    "    train, test = train_test_split(data)\n",
    "    train_samples = train[:, 1:]; train_labels = train[:,0]\n",
    "    test_samples = test[:, 1:]; test_labels = test[:, 0]\n",
    "    num_classes = 10\n",
    "    mistake_counters = init_mistake_counters(train_samples, num_classes)\n",
    "    # Pre-compute outside for speed\n",
    "    kernel_mat_train = polynomial_kernel(train_samples, train_samples, 3)\n",
    "    kernel_mat_test = polynomial_kernel(train_samples, test_samples, 3)\n",
    "    \n",
    "    \n",
    "    for epoch in range(10):\n",
    "        mistake_counters, mistakes = traingen(train_samples, train_labels, mistake_counters, num_classes, kernel_mat_train, d = 3)\n",
    "        test_rate = testclassifiers(train_samples, test_samples, test_labels, mistake_counters, num_classes, kernel_mat_test, d = 3)\n",
    "        print(f\"Epoch {epoch}, made {mistakes} mistakes on training data and mistake rate of {test_rate} on test data\")\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
